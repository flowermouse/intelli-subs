import re
import os
import math
import subprocess
import argparse
from time import sleep
from pydub import AudioSegment

SAMPLE_RATE = 24000  # edge-tts é»˜è®¤è¾“å‡º 24kHz
CHANNELS = 1

def parse_srt(file_path, merge_gap_ms=300):
    """è§£æ SRT æ–‡ä»¶ï¼Œè¿”å›å­—å¹•åˆ—è¡¨ [{'start_ms': int, 'end_ms': int, 'text': str}, ...]
       è‹¥ç›¸é‚»ä¸¤æ¡å­—å¹•çš„é—´éš” <= merge_gap_msï¼Œåˆ™åˆå¹¶ä¸ºä¸€æ¡ã€‚
    """
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    pattern = re.compile(
        r"(\d+)\s+([\d:,]+)\s+-->\s+([\d:,]+)\s+([\s\S]+?)(?=\n\n|\Z)",
        re.MULTILINE,
    )
    raw_subtitles = []

    for match in pattern.finditer(content):
        index = int(match.group(1))
        start = srt_time_to_ms(match.group(2))
        end = srt_time_to_ms(match.group(3))
        text = match.group(4).strip().replace("\n", " ")

        raw_subtitles.append(
            {"index": index, "start_ms": start, "end_ms": end, "text": text}
        )

    if not raw_subtitles:
        return []

    # åˆå¹¶é—´éš” <= merge_gap_ms çš„ç›¸é‚»å­—å¹•
    merged = []
    current = raw_subtitles[0].copy()

    for sub in raw_subtitles[1:]:
        gap = sub["start_ms"] - current["end_ms"]
        if gap <= merge_gap_ms:
            # åˆå¹¶ï¼šèµ·å§‹æ—¶é—´å–å½“å‰çš„ï¼Œç»“æŸæ—¶é—´å–åä¸€æ¡çš„ï¼Œæ–‡æœ¬æ‹¼æ¥
            current["end_ms"] = max(current["end_ms"], sub["end_ms"])
            current["text"] = current["text"].rstrip() + " " + sub["text"].lstrip()
        else:
            merged.append(current)
            current = sub.copy()

    merged.append(current)

    # é‡æ–°ç¼–å· index
    for i, sub in enumerate(merged, start=1):
        sub["index"] = i

    return merged

def srt_time_to_ms(time_str):
    """å°† SRT æ—¶é—´æ ¼å¼ (HH:MM:SS,mmm) è½¬æ¢ä¸ºæ¯«ç§’"""
    hours, minutes, seconds = time_str.split(":")
    seconds, milliseconds = seconds.split(",")
    total_ms = (
        int(hours) * 3600000
        + int(minutes) * 60000
        + int(seconds) * 1000
        + int(milliseconds)
    )
    return total_ms

def generate_audio_for_text(text, idx, voice_name="zh-CN-YunxiaoMultilingualNeural", rate=None):
    """ç”¨ edge-tts ç”ŸæˆéŸ³é¢‘ï¼Œè¿”å› mp3 æ–‡ä»¶è·¯å¾„"""
    out_mp3 = f"tmp_{idx}.mp3"
    cmd = [
        "edge-tts",
        "--voice", voice_name,
        "--text", text,
        "--write-media", out_mp3
    ]
    if rate:
        cmd.extend([f"--rate={rate}"])
    subprocess.run(cmd, check=True)
    return out_mp3

def align_and_merge_audio(subtitles, voice_name="zh-CN-YunxiaoMultilingualNeural"):
    merged = AudioSegment.silent(duration=0, frame_rate=SAMPLE_RATE)
    current_position = 0

    for i, sub in enumerate(subtitles):
        start_ms = sub["start_ms"]
        end_ms = sub["end_ms"]
        text = sub["text"]

        print(f"[{i+1}/{len(subtitles)}] ç”ŸæˆéŸ³é¢‘: {text[:30]}... ({start_ms}ms -> {end_ms}ms)")

        while True:
            try:
                mp3_path = generate_audio_for_text(text, i, voice_name)
                seg = AudioSegment.from_file(mp3_path)
                seg = seg.set_frame_rate(SAMPLE_RATE).set_channels(CHANNELS)
                break
            except Exception as e:
                print(f"   âš ï¸  ç”Ÿæˆå¤±è´¥: {e}, é‡è¯•ä¸­...")
            sleep(1)
        
        # ä¸‹ä¸€æ¡å­—å¹•çš„å¼€å§‹æ—¶é—´ - å½“å‰å­—å¹•çš„å¼€å§‹æ—¶é—´ ä½œä¸ºé˜ˆå€¼
        threshold = subtitles[i+1]["start_ms"] - start_ms if i + 1 < len(subtitles) else float('inf')
        subtitle_duration = max(1, end_ms - start_ms)  # æ¯«ç§’
        audio_duration = max(1, len(seg))  # æ¯«ç§’

        # å¦‚æœæ—¶é•¿å·®å¼‚è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™ç”¨ rate å‚æ•°é‡æ–°ç”Ÿæˆç›´åˆ°æ¥è¿‘åŒ¹é…ï¼ˆæœ€å¤šå°è¯•è‹¥å¹²æ¬¡ï¼‰
        if audio_duration > threshold or audio_duration/subtitle_duration > 1.2 or audio_duration/subtitle_duration < 0.8:

            # è®¡ç®—åˆå§‹æ‰€éœ€é€Ÿåº¦å› å­ S = audio_duration / subtitle_duration
            S = audio_duration / subtitle_duration
            # å°†å› å­è½¬ä¸º edge-tts çš„ rate ç™¾åˆ†æ¯” p
            # S=1.5 -> +50%
            # S=0.8 -> -20%
            p = int(math.ceil((S - 1) * 100))
            # é™åˆ¶ç™¾åˆ†æ¯”èŒƒå›´ï¼Œé¿å…ä¸åˆç†æ•°å€¼ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
            p = max(-50, min(150, p))
            rate_str = f"{p:+d}%"

            while True:
                print(f"   â¤ å°è¯•é€šè¿‡ edge-tts è°ƒæ•´é€Ÿç‡é‡ç”Ÿæˆï¼Œrate={rate_str}")
                try:
                    mp3_path = generate_audio_for_text(text, i, voice_name, rate_str)
                    seg = AudioSegment.from_file(mp3_path)
                    seg = seg.set_frame_rate(SAMPLE_RATE).set_channels(CHANNELS)
                    audio_duration = max(1, len(seg))
                    if audio_duration <= threshold:
                        break
                    else:
                        new_S = audio_duration / subtitle_duration
                        # é‡æ–°è®¡ç®— rate ç™¾åˆ†æ¯”ï¼Œéœ€è¦åœ¨ä¸Šä¸€è½®åŸºç¡€ä¸Šè°ƒæ•´
                        S = new_S * S  # ä¹˜ä»¥ä¸Šä¸€æ¬¡çš„ S
                        p = int(math.ceil((S - 1) * 100))
                        p = max(-50, min(150, p))
                        rate_str = f"{p:+d}%"
                except Exception as e:
                    print(f"   âš ï¸  é‡ç”Ÿæˆå¤±è´¥: {e}, é‡è¯•ä¸­...")

        # åˆ é™¤ä¸´æ—¶ mp3 æ–‡ä»¶ï¼ˆä¿å®ˆåˆ é™¤ï¼Œé¿å…æ®‹ç•™ï¼‰
        try:
            os.remove(mp3_path)
        except Exception:
            pass

        if len(seg) < threshold:
            seg += AudioSegment.silent(duration=threshold - len(seg), frame_rate=SAMPLE_RATE)
        else:
            # è¿™ä¸ªåˆ†æ”¯ä¸€èˆ¬ä¸ä¼šè§¦å‘ï¼Œå› ä¸ºä¸Šé¢å·²ç»æ§åˆ¶äº†é•¿åº¦
            seg = seg[:threshold]
        
        # æ·»åŠ æœ¬æ®µå¹¶æ¨è¿›å½“å‰ä½ç½®
        merged += seg
        current_position += len(seg)

    return merged

def save_wave(filename, audio: AudioSegment):
    audio.export(filename, format="wav")

def main():
    # srt_file = "1_zh.srt"  # æ›¿æ¢ä¸ºä½ çš„ SRT æ–‡ä»¶è·¯å¾„
    # output_file = "1.wav"
    # voice_name = "zh-CN-YunxiaoMultilingualNeural"
    parser = argparse.ArgumentParser(description="æ ¹æ® SRT æ–‡ä»¶ç”Ÿæˆé…éŸ³éŸ³é¢‘")
    parser.add_argument("--srt", required=True, help="è¾“å…¥ SRT å­—å¹•æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_file", required=True, help="è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆwav æ ¼å¼ï¼‰")
    parser.add_argument(
        "--voice_name",
        default="zh-CN-YunxiaoMultilingualNeural",
        help="edge-tts è¯­éŸ³åç§°ï¼Œé»˜è®¤ zh-CN-YunxiaoMultilingualNeural",
    )
    args = parser.parse_args()

    print(f"ğŸ“– è§£æå­—å¹•æ–‡ä»¶: {args.srt}")
    subtitles = parse_srt(args.srt)
    print(f"âœ… å…± {len(subtitles)} æ¡å­—å¹•\n")

    print("ğŸ™ï¸  å¼€å§‹ç”Ÿæˆå¹¶å¯¹é½éŸ³é¢‘...")
    merged_audio = align_and_merge_audio(subtitles, args.voice_name)

    print(f"\nğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶: {args.output_file}")
    save_wave(filename=args.output_file, audio=merged_audio)
    print("âœ… å®Œæˆï¼")

if __name__ == "__main__":
    main()