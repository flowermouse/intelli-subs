pretrained_path: ./voxcpm1.5/
train_manifest: data.jsonl
val_manifest: ""

sample_rate: 44100
batch_size: 16
grad_accum_steps: 1
num_workers: 2
num_iters: 2000
log_interval: 10
valid_interval: 1000
save_interval: 1000

learning_rate: 0.00001   # Use smaller LR for full fine-tuning
weight_decay: 0.01
warmup_steps: 100
max_steps: 2000
max_batch_tokens: 8192

save_path: ./finetune
tensorboard: ./logs

lambdas:
  loss/diff: 1.0
  loss/stop: 1.0